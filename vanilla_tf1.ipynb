{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T20:52:15.882138Z",
     "start_time": "2018-08-12T20:52:15.572812Z"
    }
   },
   "source": [
    "### Vanilla TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T01:32:12.329861Z",
     "start_time": "2018-08-13T01:32:10.666631Z"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size): # not really needed for XOR\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T01:44:41.757590Z",
     "start_time": "2018-08-13T01:44:41.196266Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden = 3\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None), name='y')\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        #stddev = 2 / np.sqrt(n_inputs)\n",
    "        #init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        #W = tf.Variable(init, name=\"weights\")\n",
    "        #b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        W = tf.Variable(tf.random_uniform([n_inputs, n_neurons], -1, 1), name = \"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name = \"biases\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else: return Z\n",
    "        \n",
    "        \n",
    "Theta1 = tf.Variable(tf.random_uniform([2,3], -1, 1), name = \"Theta1\")\n",
    "Theta2 = tf.Variable(tf.random_uniform([3,1], -1, 1), name = \"Theta2\")\n",
    "\n",
    "Bias1 = tf.Variable(tf.zeros([3]), name = \"Bias1\")\n",
    "Bias2 = tf.Variable(tf.zeros([1]), name = \"Bias2\")\n",
    "        \n",
    "\n",
    "with tf.name_scope('nn'):\n",
    "    hidden = neuron_layer(X, n_hidden, name='hidden', activation=tf.nn.sigmoid)\n",
    "    probs = neuron_layer(hidden, n_outputs, name='outputs', activation=tf.nn.sigmoid)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    mse_loss = tf.reduce_mean(tf.squared_difference(y, probs), name='loss')\n",
    "    #bin_xentropy_loss = tf.reduce_mean(( (y * tf.log(probs)) + ((1 - y) * tf.log(1.0 - probs)) ) * -1)\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(mse_loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.equal(tf.greater_equal(probs,0.5), tf.cast(y,tf.bool))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "#n_epochs = 500\n",
    "#batch_size = 4\n",
    "\n",
    "X_train = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "y_train = [[0],[1],[1],[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(500):\n",
    "        _, mse, acc = sess.run([training_op, mse_loss, accuracy], \n",
    "                               feed_dict={X: np.array(X_train), y: np.array(y_train)})\n",
    "        print(\"mse: %.4f, accuracy: %.2f\" % (mse, acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T01:40:33.495068Z",
     "start_time": "2018-08-13T01:40:31.419017Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name = 'x-input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name = 'y-input')\n",
    "\n",
    "Theta1 = tf.Variable(tf.random_uniform([2,3], -1, 1), name = \"Theta1\")\n",
    "Theta2 = tf.Variable(tf.random_uniform([3,1], -1, 1), name = \"Theta2\")\n",
    "\n",
    "Bias1 = tf.Variable(tf.zeros([3]), name = \"Bias1\")\n",
    "Bias2 = tf.Variable(tf.zeros([1]), name = \"Bias2\")\n",
    "\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "\tA2 = tf.tanh(tf.matmul(x_, Theta1) + Bias1)\n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "\tHypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)\n",
    "\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "\tcost = tf.reduce_mean(( (y_ * tf.log(Hypothesis)) + \n",
    "\t\t((1 - y_) * tf.log(1.0 - Hypothesis)) ) * -1)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "\ttrain_step = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "XOR_Y = [[0],[1],[1],[0]]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(5000):\n",
    "    sess.run(train_step, feed_dict={x_: XOR_X, y_: XOR_Y})\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch %5d' % i, end='  ')\n",
    "        #print('Hypothesis ', sess.run(Hypothesis, feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "        print('cost ', sess.run(cost, feed_dict={x_: XOR_X, y_: XOR_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T01:45:08.692781Z",
     "start_time": "2018-08-13T01:45:06.460053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.2956, accuracy: 0.50\n",
      "mse: 0.2694, accuracy: 0.50\n",
      "mse: 0.2553, accuracy: 0.50\n",
      "mse: 0.2526, accuracy: 0.75\n",
      "mse: 0.2552, accuracy: 0.50\n",
      "mse: 0.2573, accuracy: 0.50\n",
      "mse: 0.2570, accuracy: 0.50\n",
      "mse: 0.2545, accuracy: 0.50\n",
      "mse: 0.2510, accuracy: 0.50\n",
      "mse: 0.2474, accuracy: 0.50\n",
      "mse: 0.2446, accuracy: 0.50\n",
      "mse: 0.2427, accuracy: 0.75\n",
      "mse: 0.2417, accuracy: 0.50\n",
      "mse: 0.2411, accuracy: 0.50\n",
      "mse: 0.2407, accuracy: 0.50\n",
      "mse: 0.2399, accuracy: 0.50\n",
      "mse: 0.2388, accuracy: 0.50\n",
      "mse: 0.2371, accuracy: 0.75\n",
      "mse: 0.2350, accuracy: 0.50\n",
      "mse: 0.2325, accuracy: 0.50\n",
      "mse: 0.2300, accuracy: 0.50\n",
      "mse: 0.2273, accuracy: 0.50\n",
      "mse: 0.2248, accuracy: 0.50\n",
      "mse: 0.2222, accuracy: 0.50\n",
      "mse: 0.2195, accuracy: 0.75\n",
      "mse: 0.2167, accuracy: 0.75\n",
      "mse: 0.2137, accuracy: 0.75\n",
      "mse: 0.2103, accuracy: 0.75\n",
      "mse: 0.2067, accuracy: 0.75\n",
      "mse: 0.2028, accuracy: 0.75\n",
      "mse: 0.1987, accuracy: 0.75\n",
      "mse: 0.1945, accuracy: 0.50\n",
      "mse: 0.1903, accuracy: 0.50\n",
      "mse: 0.1862, accuracy: 0.50\n",
      "mse: 0.1822, accuracy: 0.50\n",
      "mse: 0.1783, accuracy: 0.50\n",
      "mse: 0.1745, accuracy: 0.50\n",
      "mse: 0.1709, accuracy: 0.50\n",
      "mse: 0.1675, accuracy: 0.50\n",
      "mse: 0.1642, accuracy: 0.50\n",
      "mse: 0.1610, accuracy: 0.50\n",
      "mse: 0.1581, accuracy: 0.50\n",
      "mse: 0.1554, accuracy: 0.50\n",
      "mse: 0.1529, accuracy: 0.50\n",
      "mse: 0.1507, accuracy: 0.50\n",
      "mse: 0.1486, accuracy: 0.50\n",
      "mse: 0.1467, accuracy: 0.50\n",
      "mse: 0.1450, accuracy: 0.75\n",
      "mse: 0.1434, accuracy: 0.75\n",
      "mse: 0.1420, accuracy: 0.75\n",
      "mse: 0.1407, accuracy: 0.75\n",
      "mse: 0.1395, accuracy: 0.75\n",
      "mse: 0.1384, accuracy: 0.75\n",
      "mse: 0.1375, accuracy: 0.75\n",
      "mse: 0.1366, accuracy: 0.75\n",
      "mse: 0.1358, accuracy: 0.75\n",
      "mse: 0.1351, accuracy: 0.75\n",
      "mse: 0.1345, accuracy: 0.75\n",
      "mse: 0.1339, accuracy: 0.75\n",
      "mse: 0.1334, accuracy: 0.75\n",
      "mse: 0.1329, accuracy: 0.75\n",
      "mse: 0.1324, accuracy: 0.50\n",
      "mse: 0.1320, accuracy: 0.50\n",
      "mse: 0.1317, accuracy: 0.50\n",
      "mse: 0.1313, accuracy: 0.50\n",
      "mse: 0.1310, accuracy: 0.50\n",
      "mse: 0.1308, accuracy: 0.50\n",
      "mse: 0.1305, accuracy: 0.50\n",
      "mse: 0.1303, accuracy: 0.50\n",
      "mse: 0.1301, accuracy: 0.50\n",
      "mse: 0.1299, accuracy: 0.50\n",
      "mse: 0.1297, accuracy: 0.50\n",
      "mse: 0.1295, accuracy: 0.50\n",
      "mse: 0.1293, accuracy: 0.50\n",
      "mse: 0.1292, accuracy: 0.50\n",
      "mse: 0.1290, accuracy: 0.50\n",
      "mse: 0.1289, accuracy: 0.50\n",
      "mse: 0.1288, accuracy: 0.50\n",
      "mse: 0.1287, accuracy: 0.50\n",
      "mse: 0.1286, accuracy: 0.50\n",
      "mse: 0.1285, accuracy: 0.50\n",
      "mse: 0.1284, accuracy: 0.50\n",
      "mse: 0.1283, accuracy: 0.50\n",
      "mse: 0.1282, accuracy: 0.50\n",
      "mse: 0.1281, accuracy: 0.50\n",
      "mse: 0.1281, accuracy: 0.50\n",
      "mse: 0.1280, accuracy: 0.50\n",
      "mse: 0.1279, accuracy: 0.50\n",
      "mse: 0.1278, accuracy: 0.50\n",
      "mse: 0.1278, accuracy: 0.50\n",
      "mse: 0.1277, accuracy: 0.50\n",
      "mse: 0.1277, accuracy: 0.50\n",
      "mse: 0.1276, accuracy: 0.50\n",
      "mse: 0.1276, accuracy: 0.50\n",
      "mse: 0.1275, accuracy: 0.50\n",
      "mse: 0.1275, accuracy: 0.50\n",
      "mse: 0.1274, accuracy: 0.50\n",
      "mse: 0.1274, accuracy: 0.50\n",
      "mse: 0.1273, accuracy: 0.50\n",
      "mse: 0.1273, accuracy: 0.50\n",
      "mse: 0.1272, accuracy: 0.50\n",
      "mse: 0.1272, accuracy: 0.50\n",
      "mse: 0.1272, accuracy: 0.50\n",
      "mse: 0.1271, accuracy: 0.50\n",
      "mse: 0.1271, accuracy: 0.50\n",
      "mse: 0.1271, accuracy: 0.50\n",
      "mse: 0.1270, accuracy: 0.50\n",
      "mse: 0.1270, accuracy: 0.50\n",
      "mse: 0.1270, accuracy: 0.50\n",
      "mse: 0.1269, accuracy: 0.50\n",
      "mse: 0.1269, accuracy: 0.50\n",
      "mse: 0.1269, accuracy: 0.50\n",
      "mse: 0.1269, accuracy: 0.50\n",
      "mse: 0.1268, accuracy: 0.50\n",
      "mse: 0.1268, accuracy: 0.50\n",
      "mse: 0.1268, accuracy: 0.50\n",
      "mse: 0.1268, accuracy: 0.50\n",
      "mse: 0.1267, accuracy: 0.50\n",
      "mse: 0.1267, accuracy: 0.50\n",
      "mse: 0.1267, accuracy: 0.50\n",
      "mse: 0.1267, accuracy: 0.50\n",
      "mse: 0.1266, accuracy: 0.50\n",
      "mse: 0.1266, accuracy: 0.50\n",
      "mse: 0.1266, accuracy: 0.50\n",
      "mse: 0.1266, accuracy: 0.50\n",
      "mse: 0.1266, accuracy: 0.50\n",
      "mse: 0.1265, accuracy: 0.50\n",
      "mse: 0.1265, accuracy: 0.50\n",
      "mse: 0.1265, accuracy: 0.50\n",
      "mse: 0.1265, accuracy: 0.50\n",
      "mse: 0.1265, accuracy: 0.50\n",
      "mse: 0.1264, accuracy: 0.50\n",
      "mse: 0.1264, accuracy: 0.50\n",
      "mse: 0.1264, accuracy: 0.50\n",
      "mse: 0.1264, accuracy: 0.50\n",
      "mse: 0.1264, accuracy: 0.50\n",
      "mse: 0.1264, accuracy: 0.50\n",
      "mse: 0.1263, accuracy: 0.50\n",
      "mse: 0.1263, accuracy: 0.50\n",
      "mse: 0.1263, accuracy: 0.50\n",
      "mse: 0.1263, accuracy: 0.50\n",
      "mse: 0.1263, accuracy: 0.50\n",
      "mse: 0.1263, accuracy: 0.50\n",
      "mse: 0.1263, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1262, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1261, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1260, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1259, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1258, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1257, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1256, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1255, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1254, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1253, accuracy: 0.50\n",
      "mse: 0.1252, accuracy: 0.50\n",
      "mse: 0.1252, accuracy: 0.50\n",
      "mse: 0.1252, accuracy: 0.50\n",
      "mse: 0.1252, accuracy: 0.50\n",
      "mse: 0.1252, accuracy: 0.50\n",
      "mse: 0.1252, accuracy: 0.50\n",
      "mse: 0.1252, accuracy: 0.75\n",
      "mse: 0.1252, accuracy: 0.75\n",
      "mse: 0.1252, accuracy: 0.75\n",
      "mse: 0.1252, accuracy: 0.75\n",
      "mse: 0.1251, accuracy: 1.00\n",
      "mse: 0.1251, accuracy: 1.00\n",
      "mse: 0.1251, accuracy: 1.00\n",
      "mse: 0.1251, accuracy: 1.00\n",
      "mse: 0.1251, accuracy: 1.00\n",
      "mse: 0.1251, accuracy: 1.00\n",
      "mse: 0.1250, accuracy: 1.00\n",
      "mse: 0.1250, accuracy: 1.00\n",
      "mse: 0.1250, accuracy: 1.00\n",
      "mse: 0.1250, accuracy: 1.00\n",
      "mse: 0.1249, accuracy: 1.00\n",
      "mse: 0.1249, accuracy: 1.00\n",
      "mse: 0.1248, accuracy: 1.00\n",
      "mse: 0.1248, accuracy: 1.00\n",
      "mse: 0.1247, accuracy: 1.00\n",
      "mse: 0.1247, accuracy: 1.00\n",
      "mse: 0.1246, accuracy: 1.00\n",
      "mse: 0.1245, accuracy: 1.00\n",
      "mse: 0.1244, accuracy: 1.00\n",
      "mse: 0.1242, accuracy: 1.00\n",
      "mse: 0.1240, accuracy: 1.00\n",
      "mse: 0.1238, accuracy: 1.00\n",
      "mse: 0.1234, accuracy: 1.00\n",
      "mse: 0.1230, accuracy: 1.00\n",
      "mse: 0.1223, accuracy: 1.00\n",
      "mse: 0.1214, accuracy: 1.00\n",
      "mse: 0.1200, accuracy: 1.00\n",
      "mse: 0.1177, accuracy: 1.00\n",
      "mse: 0.1141, accuracy: 1.00\n",
      "mse: 0.1084, accuracy: 1.00\n",
      "mse: 0.1005, accuracy: 1.00\n",
      "mse: 0.0912, accuracy: 1.00\n",
      "mse: 0.0825, accuracy: 1.00\n",
      "mse: 0.0754, accuracy: 1.00\n",
      "mse: 0.0676, accuracy: 1.00\n",
      "mse: 0.0576, accuracy: 1.00\n",
      "mse: 0.0469, accuracy: 1.00\n",
      "mse: 0.0380, accuracy: 1.00\n",
      "mse: 0.0314, accuracy: 1.00\n",
      "mse: 0.0266, accuracy: 1.00\n",
      "mse: 0.0226, accuracy: 1.00\n",
      "mse: 0.0191, accuracy: 1.00\n",
      "mse: 0.0161, accuracy: 1.00\n",
      "mse: 0.0135, accuracy: 1.00\n",
      "mse: 0.0115, accuracy: 1.00\n",
      "mse: 0.0101, accuracy: 1.00\n",
      "mse: 0.0090, accuracy: 1.00\n",
      "mse: 0.0083, accuracy: 1.00\n",
      "mse: 0.0078, accuracy: 1.00\n",
      "mse: 0.0074, accuracy: 1.00\n",
      "mse: 0.0070, accuracy: 1.00\n",
      "mse: 0.0067, accuracy: 1.00\n",
      "mse: 0.0062, accuracy: 1.00\n",
      "mse: 0.0058, accuracy: 1.00\n",
      "mse: 0.0053, accuracy: 1.00\n",
      "mse: 0.0049, accuracy: 1.00\n",
      "mse: 0.0045, accuracy: 1.00\n",
      "mse: 0.0042, accuracy: 1.00\n",
      "mse: 0.0039, accuracy: 1.00\n",
      "mse: 0.0036, accuracy: 1.00\n",
      "mse: 0.0034, accuracy: 1.00\n",
      "mse: 0.0031, accuracy: 1.00\n",
      "mse: 0.0030, accuracy: 1.00\n",
      "mse: 0.0028, accuracy: 1.00\n",
      "mse: 0.0027, accuracy: 1.00\n",
      "mse: 0.0025, accuracy: 1.00\n",
      "mse: 0.0024, accuracy: 1.00\n",
      "mse: 0.0023, accuracy: 1.00\n",
      "mse: 0.0022, accuracy: 1.00\n",
      "mse: 0.0021, accuracy: 1.00\n",
      "mse: 0.0020, accuracy: 1.00\n",
      "mse: 0.0019, accuracy: 1.00\n",
      "mse: 0.0019, accuracy: 1.00\n",
      "mse: 0.0018, accuracy: 1.00\n",
      "mse: 0.0017, accuracy: 1.00\n",
      "mse: 0.0017, accuracy: 1.00\n",
      "mse: 0.0016, accuracy: 1.00\n",
      "mse: 0.0016, accuracy: 1.00\n",
      "mse: 0.0016, accuracy: 1.00\n",
      "mse: 0.0015, accuracy: 1.00\n",
      "mse: 0.0015, accuracy: 1.00\n",
      "mse: 0.0015, accuracy: 1.00\n",
      "mse: 0.0014, accuracy: 1.00\n",
      "mse: 0.0014, accuracy: 1.00\n",
      "mse: 0.0014, accuracy: 1.00\n",
      "mse: 0.0014, accuracy: 1.00\n",
      "mse: 0.0013, accuracy: 1.00\n",
      "mse: 0.0013, accuracy: 1.00\n",
      "mse: 0.0013, accuracy: 1.00\n",
      "mse: 0.0013, accuracy: 1.00\n",
      "mse: 0.0013, accuracy: 1.00\n",
      "mse: 0.0012, accuracy: 1.00\n",
      "mse: 0.0012, accuracy: 1.00\n",
      "mse: 0.0012, accuracy: 1.00\n",
      "mse: 0.0012, accuracy: 1.00\n",
      "mse: 0.0012, accuracy: 1.00\n",
      "mse: 0.0012, accuracy: 1.00\n",
      "mse: 0.0012, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0011, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0010, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0009, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0008, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0007, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n",
      "mse: 0.0006, accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden = 3\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None), name='y')\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else: return Z\n",
    "\n",
    "with tf.name_scope('nn'):\n",
    "    hidden = neuron_layer(X, n_hidden, name='hidden', activation=tf.nn.sigmoid)\n",
    "    prediction_probabilities = neuron_layer(hidden, n_outputs, name='outputs', activation=tf.nn.sigmoid)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    mse_loss = tf.reduce_mean(tf.squared_difference(y, prediction_probabilities), name='loss')    \n",
    "    loss_summary = tf.summary.scalar('mse_loss', mse_loss)\n",
    "    \n",
    "learning_rate = 0.1\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(mse_loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.equal(tf.greater_equal(prediction_probabilities,0.5), tf.cast(y,tf.bool))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X_train = [\n",
    "    (0, 0),\n",
    "    (0, 1),\n",
    "    (1, 0),\n",
    "    (1, 1)\n",
    "]\n",
    "y_train = [[0],[1],[1],[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(500):\n",
    "        _, mse, acc = sess.run([training_op, mse_loss, accuracy], \n",
    "                               feed_dict={X: np.array(X_train), y: np.array(y_train)})\n",
    "        print(\"mse: %.4f, accuracy: %.2f\" % (mse, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIST OF TF TO DOS:\n",
    "- minimal example (no classes, saves, etc)\n",
    "- classes, batches w/ variable size\n",
    "- saves, tb, summaries\n",
    "- eager"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48.8px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
